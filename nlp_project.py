# -*- coding: utf-8 -*-
"""NLP project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oXG2jDo5TufPhfcz731TUn0_G1TZxtDU
"""

import os
import numpy as np
import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import regex as re
from string import punctuation
import math

import nltk
nltk.download("omw-1.4")
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer #converting text to numerical form
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

import pandas as pd

# Selecting a subset of data to be faster in demonstration
data = pd.read_csv(r'/content/IMDB Dataset.csv')

data.head(10)

data.describe() #418 Duplicates

"""DATA CLEANING"""

data[data['review'].duplicated() == True]

data.drop_duplicates(subset='review', inplace=True)

data.describe()

def remove_punc(series):
    temp = re.sub(f'[{punctuation}]', '', series)
    temp = re.sub(' br br ','', temp)
    temp = re.sub(' n ','', temp)
    return temp

data['review'] = data['review'].apply(remove_punc)

data[['review']].head()

def remove_stop(series):
    return ' '.join([x.lower() for x in series.split(' ') if x.lower() not in STOPWORDS])

data['review'] = data['review'].apply(remove_stop)

data[['review']]

import nltk
nltk.download('punkt_tab')

def tokenize(series):
    return word_tokenize(series)

data['tokens'] = data['review'].apply(tokenize)

def get_len(series):
    return len(series)

data['token_len'] = data['tokens'].apply(get_len)

data[['tokens','token_len']].head()

data.describe()

MAX_LEN = math.ceil(data.describe().values[1])
print(MAX_LEN)

def pad_token(series):
    if len(series) < MAX_LEN:
        series.extend(['<END>']*(MAX_LEN-len(series)))
        return series
    else:
        return series[:MAX_LEN]

data['paded_tokens'] = data['tokens'].apply(pad_token)

print(data['paded_tokens'].values[10])

data[['tokens','paded_tokens']]

import nltk
nltk.download('wordnet')

"""**NORMALAIZATION**

LEMMATAIZATION
"""

lemmatizer = WordNetLemmatizer()

def lemma(series):
    return [lemmatizer.lemmatize(word) for word in series]

data['lemma_tokens'] = data['paded_tokens'].apply(lemma)

data[['tokens','lemma_tokens']]

"""STEMMING"""

stemmer = PorterStemmer()

def stem(series):
    return [stemmer.stem(word) for word in series]

data['stem_tokens'] = data['tokens'].apply(stem)

data[['tokens','stem_tokens']]

"""POS TAGGING"""

import nltk
nltk.download('averaged_perceptron_tagger_eng')
import nltk
nltk.download('universal_tagset')

def pos_t(series):
    return nltk.pos_tag(series, tagset='universal')

data['pos_tag_tokens'] = data['tokens'].apply(pos_t)

data[['tokens','pos_tag_tokens']]

"""WORD EMBEDDING

"""

unique_words = set()
for tokens in list(data['lemma_tokens'].values):
    unique_words.update(tokens)

print('Count of Unique words:', len(unique_words))

word2idx = {}
for word in unique_words:
    word2idx[word] = len(word2idx)
word2idx['<END>'] = len(word2idx)

word_embeddings = np.random.rand(len(word2idx),200)
with open('/content/glove.6B.100d.txt', 'r') as embeds:
    embeddings = embeds.read()
    embeddings = embeddings.split('\n')[:-2]

for token_idx, token_embed in enumerate(embeddings):
    token = token_embed.split()[0]
    if token in word2idx:
        word_embeddings[word2idx[token]] = [float(val) for val in token_embed.split()[1:]]

print(f'Word embeddings for word {list(word2idx.keys())[300]}:',word_embeddings[300])

"""VECTORIZATION

"""

vectorizer = CountVectorizer()
vector_cl = vectorizer.fit_transform([' '.join(tok) for tok in list(data['lemma_tokens'].values)])
vector_cl.shape

def label_vec(series):
    return int(series=='positive')

data['sentiment'] = data['sentiment'].apply(label_vec)
data[['lemma_tokens', 'sentiment']].head()

"""IDENTIFY AND TRAIN MODEL"""

vector_cl_train_x, vector_cl_test_x, vector_cl_train_y, vector_cl_test_y = train_test_split(vector_cl, data['sentiment'].values, test_size=0.3, random_state=42)
print(vector_cl_train_x.shape,vector_cl_train_y.shape,vector_cl_test_x.shape,vector_cl_test_y.shape)

log_reg = LogisticRegression().fit(vector_cl_train_x, vector_cl_train_y)

pred = log_reg.predict(vector_cl_test_x)
print('Mean Accuracy:', log_reg.score(vector_cl_test_x, vector_cl_test_y))
print('F1 Score:', f1_score(vector_cl_test_y, pred))

review = input("give your feedback : ")  #'This movie was one of the best i watched in recent times' ::'This movie was not bad and i really liked it.'::'The cinematics of this movie made my eyes bleed'
print(log_reg.predict(vectorizer.transform([review])))

# prompt: install gradio in system

!pip install gradio

import gradio as gr
import os
import numpy as np
import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import regex as re
from string import punctuation
import math
import nltk
from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

# Download necessary NLTK data (only needed once)
nltk.download("omw-1.4", quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('averaged_perceptron_tagger_eng', quiet=True)
nltk.download('universal_tagset', quiet=True)


# ... (rest of your existing code for data preprocessing and model training) ...

# Assuming your trained model is 'log_reg' and vectorizer is 'vectorizer'


def predict_sentiment(review):
    prediction = log_reg.predict(vectorizer.transform([review]))
    if prediction[0] == 1:
      return "Positive"
    else:
      return "Negative"

iface = gr.Interface(
    fn=predict_sentiment,
    inputs=gr.Textbox(lines=5, placeholder="Enter your movie review here..."),
    outputs="text",
    title="Movie Review Sentiment Analysis",
    description="Enter a movie review, and the model will predict if it's positive or negative.",
)

iface.launch()

"""WORDCLOUD"""

import os
import numpy as np
import pandas as pd
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# ... (your other import statements)

# Load the dataset
data = pd.read_csv('/content/IMDB Dataset.csv') # Make sure this path is correct

# Now you can use 'data' in your wordcloud generation
words = ' '.join(data['review'].values)
wordcloud = WordCloud(width = 800, height = 800,
                background_color ='white',
                stopwords = STOPWORDS,
                min_font_size = 10).generate(words)

plt.imshow(wordcloud)
plt.axis("off")
plt.show()









